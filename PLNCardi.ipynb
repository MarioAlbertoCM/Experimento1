{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f35ffc-68c8-4dbc-b06a-d433827318fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61dbb95-8510-4c38-a26d-ce29acabbd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mcm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#libreria tokenizador\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "import re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6830a23-dd7f-4b51-a6d4-6bfc52c86eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mcm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#libreria palabras vacias\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "palabras_vacias=set(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8951ccb9-164b-43dd-84c4-e02d8b9e5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "import spacy\n",
    "lematiza = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87676500-e8ad-4762-83f9-781432b769ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steamming\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d08646-ef78-4b18-b62c-97d7de8e5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Cardi=pd.read_csv('cardiUnificado.csv', encoding='utf-8')\n",
    "textos=df_Cardi['texto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7697a2-1037-4169-b6d9-144826335c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lema=pd.DataFrame({'texto': []})\n",
    "df_Stemmer=pd.DataFrame({'texto': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87bf1d94-dad2-4652-bba3-330c0e34a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- Linea 1 ---------------------------------- \n",
      "--Elimina @user\n",
      "--Elimina caracteres especiales [\\.$\\*¿\\?¡!\\+\\-\\[\\]\\)\\(\\\\]\",\" \n",
      "--Retira hashtags\n",
      "--Elimina correos electronicos\n",
      "-- Separa palabras con notacion Camell\n",
      "       ---- Ninguna palabra encontrada\n",
      "-- Lista frases que inicias con \"en\" \n",
      "       ---- Frase encontrada[]\n",
      "-- Normalizar a minusculas\n",
      "       ----  jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde perú\n",
      "-- Elimina palabras vacías\n",
      "       ----  jajajaja dale, hacete boluda vos jajaja igual vos nunca puede tomar serio mando abrazo perú\n",
      "-- Lematiza\n",
      "       ----  --> \n",
      "       ---- jajajaja-->jajajaja\n",
      "       ---- dale-->dale\n",
      "       ---- ,-->,\n",
      "       ---- hacete-->hacetar\n",
      "       ---- boluda-->boluda\n",
      "       ---- vos-->vo\n",
      "       ---- jajaja-->jajaja\n",
      "       ---- igual-->igual\n",
      "       ---- vos-->vo\n",
      "       ---- nunca-->nunca\n",
      "       ---- puede-->poder\n",
      "       ---- tomar-->tomar\n",
      "       ---- serio-->serio\n",
      "       ---- mando-->mando\n",
      "       ---- abrazo-->abrazo\n",
      "       ---- perú-->perú\n",
      "       -->perú,abrazo,mando,serio,tomar,poder,nunca,vo,igual,jajaja,vo,boluda,hacetar,,,dale,jajajaja, ,\n",
      "-- Stemmer - Raíz de la palabra\n",
      "       ---- jajajaj,dal,,,hacet,bolud,vos,jajaj,igual,vos,nunc,pued,tom,seri,mand,abraz,peru\n",
      "Se procesaron: 3033\n"
     ]
    }
   ],
   "source": [
    "lineaLeida=0\n",
    "cammel=0\n",
    "log=1\n",
    "cadenalema=''\n",
    "\n",
    "for texto in textos:\n",
    "    lineaLeida+=1\n",
    "    if(lineaLeida<=log):\n",
    "        print(\"---------------------------------- Linea \"+ str(lineaLeida) +\" ---------------------------------- \")\n",
    "        #eliminar menciones de @user\n",
    "        print('--Elimina @user')\n",
    "    texto=re.sub('@user',\"\",texto)\n",
    "    #Eliminar caracteres especiales: + / ? ! ¡ ...\n",
    "    if(lineaLeida<=log):\n",
    "        print('--Elimina caracteres especiales [\\.$\\*¿\\?¡!\\+\\-\\[\\]\\)\\(\\\\\\]\",\" ')\n",
    "    texto=re.sub(\"[\\.$\\*¿\\?¡!\\+\\-\\[\\]\\)\\(\\\\\\]\",\"\",texto)\n",
    "    #Para los hashtags, solo eliminar el símbolo (#)\n",
    "    if(lineaLeida<=log):\n",
    "        print('--Retira hashtags')\n",
    "    texto=re.sub(\"[#]\",\"\",texto)\n",
    "    #Eliminar correos electrónicos.\n",
    "    if(lineaLeida<=log):\n",
    "        print('--Elimina correos electronicos')\n",
    "    texto= re.sub(\"(\\S+@\\S+\\.\\S)\", \"\", texto)\n",
    "    \n",
    "    # Separación de palabras en notación Camell: “QuedateEnCasa→ Quedate En Casa”.\n",
    "    if(lineaLeida<=log):\n",
    "        print('-- Separa palabras con notacion Camell')\n",
    "    patron=re.compile('([ÁÉÍÓÚA-Z]*[a-z0-9]+[ÁÉÍÓÚA-Z]+)+')\n",
    "    for palabra in texto.split(' '):\n",
    "        if patron.match(palabra)!=None:\n",
    "            palabraSep = \" \".join(re.findall('[ÁÉÍÓÚA-Z]+[a-z0-9]*',palabra))\n",
    "            texto=re.sub(palabra,palabraSep,texto)\n",
    "            if(lineaLeida<=log):\n",
    "                print('       ---- Palabra encontrada: ' + palabraSep)\n",
    "            cammel=+1\n",
    "    if(cammel<log):\n",
    "        if(lineaLeida<=log):\n",
    "            print('       ---- Ninguna palabra encontrada')\n",
    "    cammel=0\n",
    "    \n",
    "    \n",
    "    #Extraer/listar todas las frases que inicien con la preposición “en”, seguido de espacio en blanco y luego una palabra que inicie en Mayúsculas.\n",
    "    if(lineaLeida<=log):\n",
    "        print('-- Lista frases que inicias con \"en\" ')\n",
    "        print('       ---- Frase encontrada' + str(re.findall('\\sen\\s[A-Z]\\S+',texto)))\n",
    "    \n",
    "    #Normalizar todo a minúsculas.\n",
    "    if(lineaLeida<=log):\n",
    "        print('-- Normalizar a minusculas')\n",
    "    \n",
    "    texto = texto.lower()\n",
    "    \n",
    "    if(lineaLeida<=log):\n",
    "        print('       ---- '+ texto)\n",
    "    \n",
    "    #Eliminar palabras vacías.\n",
    "    if(lineaLeida<=log):\n",
    "        print('-- Elimina palabras vacías')\n",
    "        \n",
    "    texto=\" \".join([palabra for palabra in texto.split(' ') if palabra not in palabras_vacias])\n",
    "    \n",
    "    if(lineaLeida<=log):\n",
    "        print('       ---- '+ texto)\n",
    "    \n",
    "    #Lematizar\n",
    "    if(lineaLeida<=log):\n",
    "        print('-- Lematiza')\n",
    "    \n",
    "    textol=lematiza(texto)\n",
    "    \n",
    "    for palabra in textol:\n",
    "        lema= palabra.lemma_\n",
    "        palabra_original=palabra.text\n",
    "        cadenalema=lema+','+cadenalema\n",
    "        if(lineaLeida<=log):\n",
    "            print('       ---- '+palabra_original+'-->'+lema)\n",
    "    \n",
    "    if(lineaLeida<=log):\n",
    "        print('       -->'+cadenalema)\n",
    "    \n",
    "    #------------------------------------------------------------\n",
    "    df_nuevafila = pd.DataFrame({'texto': [cadenalema]})\n",
    "    df_lema = pd.concat([df_lema, df_nuevafila], ignore_index=True)\n",
    "    cadenalema=''\n",
    "    #------------------------------------------------------------\n",
    "            \n",
    "    #Stemmer\n",
    "    if(lineaLeida<=log):\n",
    "        print('-- Stemmer - Raíz de la palabra')\n",
    "    s=word_tokenize(texto)\n",
    "    esp=SnowballStemmer('spanish')\n",
    "    text_r=[esp.stem(token) for token in s]\n",
    "    str_text_r = ','.join(text_r)    \n",
    "        \n",
    "    if(lineaLeida<=log):\n",
    "        print('       ---- '+str(str_text_r))\n",
    "    \n",
    "    #------------------------------------------------------------\n",
    "    df_nuevafila = pd.DataFrame({'texto': [str_text_r]})\n",
    "    df_Stemmer = pd.concat([df_Stemmer, df_nuevafila], ignore_index=True)\n",
    "    str_text_r=''\n",
    "    #------------------------------------------------------------\n",
    "        \n",
    "print(\"Se procesaron: \" + str(lineaLeida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0ccb6a8-0009-428b-8a56-7d62154e9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddae770-553f-4c47-ae98-f36bf7bc3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto TfidfVectorizer\n",
    "vectorizerLema = TfidfVectorizer()\n",
    "vectorizerStem = TfidfVectorizer()\n",
    "\n",
    "# Ajustar el vectorizador en los datos de entrada\n",
    "vectorizerLema.fit(df_lema['texto'])\n",
    "vectorizerStem.fit(df_Stemmer['texto'])\n",
    "\n",
    "# Transformar los datos de entrada en la matriz TF-IDF\n",
    "tfidf_matrixLema = vectorizerLema.transform(df_lema['texto'])\n",
    "tfidf_matrixStem = vectorizerStem.transform(df_Stemmer['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5ecaad-fb04-4e35-b080-4e5ec480d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el diccionario de términos del vectorizador\n",
    "terms_dictLema = vectorizerLema.vocabulary_\n",
    "terms_dictStem = vectorizerStem.vocabulary_\n",
    "\n",
    "# Obtener los nombres de los términos a partir del diccionario de términos\n",
    "feature_namesLema = [term for term, index in sorted(terms_dictLema.items(), key=lambda x: x[1])]\n",
    "feature_namesStem = [term for term, index in sorted(terms_dictStem.items(), key=lambda x: x[1])]\n",
    "\n",
    "# Convertir la matriz de TF-IDF en un DataFrame de pandas\n",
    "tfidf_dfLema = pd.DataFrame(tfidf_matrixLema.todense(), columns=feature_namesLema)\n",
    "tfidf_dfStem = pd.DataFrame(tfidf_matrixStem.todense(), columns=feature_namesStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0273b-39cd-4e21-a161-490f23afe1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dfLema.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62048709-1f9f-46a6-ab84-ad8f5fe75f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dfStem.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3953b1c-8b37-4c91-9e88-193cb3d58352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregar columna de id a los 2 datasets tfidf\n",
    "tfidf_dfLema['Indice']=tfidf_dfLema.index\n",
    "tfidf_dfStem['Indice']=tfidf_dfStem.index\n",
    "\n",
    "#agregar la columna de id al dataset que contiene las etiquetas\n",
    "#cambiamos el nombre de etiqueta para evitar duplicado con otra columna\n",
    "df_Cardi['Indice']=df_Cardi.index\n",
    "df_Cardi['Trg']=df_Cardi['etiqueta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad138ed-37bd-4a94-aa73-4ecca6f63403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>ñoño</th>\n",
       "      <th>órgano</th>\n",
       "      <th>últimamente</th>\n",
       "      <th>último</th>\n",
       "      <th>últimodíadenavidad</th>\n",
       "      <th>único</th>\n",
       "      <th>úselo</th>\n",
       "      <th>útil</th>\n",
       "      <th>Indice</th>\n",
       "      <th>Trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7419 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01   02   03   04   10  100  1000  10000  ...  ñoño  órgano  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...   0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...   0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...   0.0     0.0   \n",
       "\n",
       "   últimamente  último  últimodíadenavidad  único  úselo  útil  Indice  Trg  \n",
       "0          0.0     0.0                 0.0    0.0    0.0   0.0       0    0  \n",
       "1          0.0     0.0                 0.0    0.0    0.0   0.0       1    1  \n",
       "2          0.0     0.0                 0.0    0.0    0.0   0.0       2    2  \n",
       "\n",
       "[3 rows x 7419 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hacer join/merge con tfidf Lema y el dataset original que tiene las etiquetas\n",
    "#Los campos en lema son en minusculas y el campo Indice con I mayuscula\n",
    "tfidf_dfLema=pd.merge(tfidf_dfLema,df_Cardi[['Trg','Indice']], on='Indice')\n",
    "tfidf_dfLema.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07797e8a-c4b6-425b-b487-78390859bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>zonz</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zorr</th>\n",
       "      <th>zum</th>\n",
       "      <th>zumbadisim</th>\n",
       "      <th>zurd</th>\n",
       "      <th>ño</th>\n",
       "      <th>ñoñ</th>\n",
       "      <th>Indice</th>\n",
       "      <th>Trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5828 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01   02   03   04   10  100  1000  10000  ...  zonz  zoo  zorr  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...   0.0  0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...   0.0  0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...   0.0  0.0   0.0   \n",
       "\n",
       "   zum  zumbadisim  zurd   ño  ñoñ  Indice  Trg  \n",
       "0  0.0         0.0   0.0  0.0  0.0       0    0  \n",
       "1  0.0         0.0   0.0  0.0  0.0       1    1  \n",
       "2  0.0         0.0   0.0  0.0  0.0       2    2  \n",
       "\n",
       "[3 rows x 5828 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hacer join/merge con tfidf Stemmer y el dataset original que tiene las etiquetas\n",
    "#Los campos en lema son en minusculas y el campo Indice con I mayuscula\n",
    "tfidf_dfStem=pd.merge(tfidf_dfStem,df_Cardi[['Trg','Indice']], on='Indice')\n",
    "tfidf_dfStem.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7372e6f-3049-43a9-9ac1-e03bdf9f6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea el archivo Lema TF-IDF\n",
    "tfidf_dfLema.to_csv('tfidf_dfLema.csv',index=False)\n",
    "# crea el archivo Stemmer TF-IDF\n",
    "tfidf_dfStem.to_csv('tfidf_dfStem.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
